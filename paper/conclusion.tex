% !TeX root = thesis.tex

\chapter{Conclusion}
\label{ch:conclusion}

The main objective of this thesis was to study different approaches to optimise the test suite of a typical software project. Three approaches have been introduced to this extend: \tsm{}, \tcs{} and \tcp{}. We have successfully implemented the latter approach using the \velocity{} framework. Furthermore, this framework features the Alpha algorithm as a novel prioritisation algorithm. The performance of the Alpha algorithm has been evaluated, mainly on the UGent Dodona project. The results are promising, resulting in \SI{95}{\percent} fewer executed test cases and \SI{97}{\percent} less time spent waiting for the first test case to fail.\\

\noindent The second purpose of this thesis was to gain useful insights into the characteristics of a regular test suite, formalised into three research questions. The first question was to estimate the expected failure probability of a test run. To answer this question, we have analysed more than 28 million test runs on Travis-CI. This analysis has indicated that \SI{18}{\percent} of those test runs have failed. Additionally, we have used this dataset to answer another question, which was to determine the typical duration of a test run. Statistical analysis has revealed that developers mainly use Travis-CI for small projects, with an average test suite duration of seven minutes. \SI{0.20}{\percent} of the test suites take longer than one hour to execute, and some projects use mutation testing. The final question was to examine the probability of consecutive failing test runs. This probability was estimated at \SI{52}{\percent} using a second Travis-CI dataset from the TravisTorrent project\cite{msr17challenge}.

\clearpage

\section{Future work}
The proposed architecture currently features a Java agent, which supports the prediction of Gradle projects using ten available predictors. However, there is still room for improvements. The paragraphs below will suggest some ideas for possible enhancements.

\subsection{Java agent}
We can extend the functionality of the Java agent in multiple ways. Its current biggest weakness is the lack of support for parallel test case execution. To allow parallel testing, we must first solve a problem related to the scheduling process. Since the execution time of a test case can vary significantly, a coordination mechanism is required to schedule which test case should be executed on which thread. One possibility would be to consider the average execution time per test case, which we can obtain by examining prior runs. Alternatively, the scheduling can be performed at runtime using an existing inter-thread communication paradigm, such as message passing. Specific to the Java agent, implementing parallel execution requires us to modify the current \texttt{TestProcessor} to extend the \texttt{MaxNParallelTestClassProcessor} instead. A thread pool should ideally be used to diminish the overhead of restarting a new thread for every test case.

\subsection{Predictions}
We can make four different enhancements to the predictors.\\

\noindent For the first enhancement, the predictor should be able to discriminate between a unit test, an integration test or a regression test. Recall that the scope of a unit test is limited to a small fraction of the application code and that its execution time is usually low. Contrarily, an integration test and a regression test usually take much longer to execute and test multiple components of the application at once. The predictor should ideally make use of this distinction and assume that a failing unit test will almost certainly affect a regression or integration test as well. Consequently, the predictor should prioritise unit tests over other types of tests.\\

\noindent Secondly, the prediction algorithms currently take into account which source code lines have either been modified or removed to identify which test cases have been affected. Likewise, a change in the code of the test case itself should also consider that test case affected, as the change might have introduced a bug as well.\\

\noindent A third possible improvement would be to examine the performance of combining multiple prediction algorithms. Currently, the algorithms operate independently from each other, but there might be hidden potential in combining the individual strengths of these algorithms dynamically at runtime. A simple implementation is possible by modifying the existing meta predictor. Instead of assigning a score to the entire prediction, we could combine several predictions using predefined weights from earlier predictions.\\

\noindent Finally, the predictors do not currently consider branch coverage in addition to statement coverage. Not every coverage tool is capable of accurately reporting which branches have been covered, therefore this has not been implemented. Branch coverage can alternatively be supported by instrumenting the source code and rewriting every conditional expression as separate \texttt{if}-statements.

\subsection{Meta predictor}
The current implementation of the meta predictor increments the score of the predictor if the prediction was above-average, and decreases the score otherwise. However, a possible problem with this approach is that the nature of the source code might evolve and change as time progresses. As a result, it might take several test suite invocations for the meta predictor to prefer an alternative predictor. We can mitigate this effect if we would use a saturating counter (\Cref{fig:saturating-counter}) instead. This idea is also used in branch predictors of microprocessors and allows a more versatile meta predictor.

\begin{figure}[htbp!]
	\centering
	\input{assets/tikz/saturating-counter.tikz}
	\caption{Saturating counter with three states}
	\label{fig:saturating-counter}
\end{figure}

\noindent In addition to implementing a different update strategy, it might be worth to investigate the use of machine learning or linear programming models as a meta predictor, or even as a prediction algorithm.

\subsection{Final enhancements}
Finally, since we can apply every implemented algorithm to \tsm{} as well, we might extend the architecture to support this technique explicitly. Executing fewer test cases will result in even lower execution times.\\

\noindent Support for new programming languages and frameworks is possible by implementing a new agent. A naive implementation would be to restart the test suite after every executed test case, should test case reordering not be supported natively by the test framework.