% !TeX root = ../thesis.tex

\section{Pipeline}
\label{sec:velocity-pipeline}
This section will elaborate on the individual steps of the pipeline. We will review these steps by manually applying the pipeline on a hypothetical Java project. The very first prediction of a project will not use the meta predictor since this requires a previous prediction. Therefore, for the sake of simplicity, we will assume a steady-state situation, which implies that we have previously executed the prediction at least once.

\subsection{Initialisation}\label{ssec:pipeline-initialisation}
In order to integrate \velocity{} in an existing Gradle project, we must modify the build script (\texttt{build.gradle}) in two places. The first change is to include and apply the plugin in the header of the file. Afterwards, we must configure the following three properties:
\begin{itemize}
	\item \texttt{base}: the path to the Java source files, relative to the location of the build script. This path will typically resemble \texttt{src/main/java}.
	
	\item \texttt{repository}: the URL to the git repository that hosts the project. This is required in subsequent steps of the pipeline, to detect which code lines have been changed in the commit currently being analysed.
	
	\item \texttt{server}: the URL to the controller.
\end{itemize}

\noindent \Cref{lst:pipeline-buildgradle} contains a minimal integration of the agent in a Gradle build script. The controller is hosted at the same machine as the agent and is reachable at port \texttt{8080}.

\lstinputlisting[caption=Minimal Gradle buildscript, label=lst:pipeline-buildgradle, language=Groovy]{assets/listings/build.gradle}

\noindent After we have configured the build script, we can execute the test suite. For the Gradle agent, this involves executing the \texttt{velocity} task, which commences the pipeline. This task requires an additional argument to be passed, which is the commit hash of the changeset to prioritise. In every discussed CI system, this commit hash is available as an environment variable.\\

\noindent The first step in the pipeline is for the agent to initiate a new test run at the controller. The agent can accomplish this by sending a \texttt{POST}-request to the \texttt{/runs} endpoint of the controller, which will reply with an identifier. On the controller side, this request will enqueue a new prioritisation request in the database. At some point in time, the controller, which is continuously polling the database, will see the request and process it in the next step.

\subsection{Prediction}
The predictor is, as was mentioned before, continuously monitoring the database for unpredicted test runs. When a new test run is detected, the predictor will execute every available prediction algorithm in order to obtain multiple prioritised test sequences. The following algorithms are available:

\paragraph*{AllInOrder} The first algorithm will generate a straightforward execution sequence by ranking every test case alphabetically. We will mainly use this sequence for benchmarking purposes in \cref{ch:evaluation}.

\paragraph*{AllRandom} We will use the second algorithm for benchmarking purposes as well. This algorithm will shuffle the list of test cases and return an arbitrary ranking.

\paragraph*{AffectedRandom} This algorithm is similar to the previous algorithm, but it will only consider the test cases that cover modified source code lines. These test cases will be ordered arbitrarily, followed by the other test cases in the test suite in no particular order.

\paragraph*{GreedyCoverAll} This algorithm is the first of three implementations of the Greedy algorithm (\cref{ssec:alg-greedy}). This variant will execute the standard algorithm on the entire test suite.

\paragraph*{GreedyCoverAffected} As opposed to the previous greedy algorithm, the second Greedy algorithm will only consider test cases covering changed source code lines. After it has prioritised these test cases, it will order the remaining test cases in the test suite randomly.

\paragraph*{GreedyTimeAll} Instead of greedily attempting to cover as many lines of the source code using as few tests as possible, this implementation will attempt to execute as many tests as possible, as soon as possible. In other words, this algorithm will prioritise test cases based on their average execution time.

\paragraph*{HGSAll} This algorithm is an implementation of the algorithm presented by Harrold, Gupta and Soffa (\cref{ssec:alg-hgs}). Similar to the \texttt{GreedyCoverAll} algorithm, this algorithm will prioritise every test case in the test suite.

\paragraph*{HGSAffected} This algorithm is identical to the previous \texttt{HGSAll} algorithm, apart from the fact that it will only prioritise test cases covering changed source code lines.

\paragraph*{ROCKET} The penultimate algorithm is a straightforward implementation of the pseudocode provided in \cref{ssec:alg-rocket}.

\paragraph*{Alpha} The final algorithm is a custom algorithm inspired by the other implemented algorithms. \Cref{sec:velocity-alpha} will further elaborate on the details.\\

\noindent Afterwards, the predictor will apply the meta predictor to determine the final prioritisation sequence. In its most primitive form, we can compare the meta predictor to a table which assigns a score to every algorithm. This score reflects the performance of the algorithm on this particular project. \Cref{ssec:pipeline-postanalysis} will explain how this score is updated. Eventually, the meta predictor will elect the sequence of the algorithm with the highest score as the final prioritised order and persist this to the database.

\subsection{Test case execution}
While the predictor is determining the test execution order, the agent will poll the controller using the previously acquired identifier by sending a \texttt{GET} request to the /runs/id endpoint. Should the prioritisation order already be available, the controller will return this. One of the discussed features of Gradle in \cref{ssec:relatedwork-gradle-junit} was the possibility to execute test cases in a chosen order by adding annotations. However, we cannot use this feature to implement the Java agent, since it only supports ordering test cases within the same test class. In order to allow complete control over the order of execution, we require a custom \texttt{TestProcessor} and \texttt{TestListener}.\\

\noindent A \texttt{TestProcessor} is responsible for processing every test class in the classpath and forwarding it, along with configurable options, to a delegate processor. The final processor in the chain will eventually perform the actual execution of the test cases. By default, the built-in processors will execute every test case in the test class it receives. Every built-in processor will, by default, immediately execute every test case in the test class it processes. Since we want to execute test cases across test classes, the custom processor needs to work differently. The provided implementation of the agent will first load every received test class to obtain all test cases in the class using reflection. Afterwards, it will store every test case in a list and iterate this list in the prioritised order. For every test case $t$ in the list, the custom processor will call the delegate processor with a tuple containing the test class and an array of options. This array will exclude every test case in the class except for $t$. This practice will forward the same test class to the delegate processor multiple times using a different option that restricts test execution to the chosen test case, resulting in the desired behaviour.\\

\noindent Furthermore, the agent calls a custom \texttt{TestListener} before and after every executed test case. This listener allows the agent to calculate the duration of the test case, as well as collect the intermediary coverage and save this on a per-test case basis.

\subsection{Post-processing and analysis}\label{ssec:pipeline-postanalysis}
The final step of the pipeline is to provide feedback to the controller to evaluate the accuracy of the predictions and thereby implementing the fourth design goal of self-improvement. After the agent has finished executing all the test cases, it will send the test results, the execution time and the coverage per test case to the controller by issuing a \texttt{POST} request to \texttt{/runs/id/test-results} and \texttt{/runs/id/coverage}.\\

\noindent When the controller receives this feedback information, it will update the meta predictor as follows. If every test case has passed, we do not update the meta predictor. The explanation for this choice is obvious. Since the objective of \tcp{} is to detect failures as fast as possible, every prioritised sequence is equally good if there are no failures at all. On the other hand, if a test case did fail, the meta predictor will inspect the predicted sequences. For every sequence, the meta predictor will calculate the duration until the first failed test case. Subsequently, it calculates the average of all these durations. Finally, the meta predictor will update the score of every algorithm by comparing its duration until the first failed test case to the average duration. For every algorithm that has a below-average duration, we increase the score and else decrease it. This process will eventually lead to the most accurate algorithms having a higher score, and these will, therefore, be preferred in following test runs.